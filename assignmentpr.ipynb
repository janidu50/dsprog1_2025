{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647d1d4e",
   "metadata": {},
   "source": [
    "  # 最終課題 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9bb963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time # It's polite to pause between requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3188af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The starting URL and the domain we must stay within\n",
    "BASE_URL = \"https://www.musashino-u.ac.jp/\"\n",
    "DOMAIN = urlparse(BASE_URL).netloc  # This will be \"www.musashino-u.ac.jp\"\n",
    "\n",
    "# This is the dictionary for the final output (key: URL, value: Title)\n",
    "sitemap = {}\n",
    "\n",
    "# This set stores URLs we've already visited or added to the queue,\n",
    "# to prevent re-crawling and getting into loops.\n",
    "visited_urls = set()\n",
    "\n",
    "# This list will act as a \"queue\" of URLs we need to crawl.\n",
    "# We start by adding the base URL.\n",
    "urls_to_crawl = [BASE_URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c65978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://www.musashino-u.ac.jp/\n",
      "--- CRAWLING COMPLETE ---\n",
      "Found pages:\n",
      "{'https://www.musashino-u.ac.jp/': 'æ\\xad¦è\\x94µé\\x87\\x8eå¤§å\\xad¦'}\n"
     ]
    }
   ],
   "source": [
    "# We'll limit the crawl to 100 pages to prevent it from running forever\n",
    "# You can change or remove this limit.\n",
    "MAX_PAGES = 100 \n",
    "\n",
    "while urls_to_crawl and len(sitemap) < MAX_PAGES:\n",
    "    # 1. Get the next URL from the queue\n",
    "    current_url = urls_to_crawl.pop(0) # .pop(0) gets from the front (Queue)\n",
    "    \n",
    "    # 2. Check if we've already processed this URL\n",
    "    if current_url in visited_urls:\n",
    "        continue\n",
    "        \n",
    "    # 3. Mark it as visited\n",
    "    visited_urls.add(current_url)\n",
    "\n",
    "    # 4. Fetch and Parse the Page (See Step 5)\n",
    "    print(f\"Crawling: {current_url}\") # Good for seeing progress\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(current_url, timeout=5)\n",
    "        # Be polite! Wait a moment before the next request\n",
    "        time.sleep(0.5) \n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            continue # Skip if the page is broken (404, 500, etc.)\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # 5. Extract the Title (as required by the assignment)\n",
    "        title = \"No Title Found\" # Default\n",
    "        if soup.title and soup.title.string:\n",
    "            title = soup.title.string.strip() # .strip() removes whitespace\n",
    "\n",
    "        # 6. Store in the dictionary\n",
    "        sitemap[current_url] = title\n",
    "\n",
    "        # 7. Find all new links (See Step 6)\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in all_links:\n",
    "            href = link['href']\n",
    "            \n",
    "            # Create an absolute URL (e.g., turn \"/about\" into \"https://.../about\")\n",
    "            full_url = urljoin(BASE_URL, href)\n",
    "            \n",
    "            # --- This is the filtering logic ---\n",
    "            \n",
    "            # 1. Check if it's in the same domain\n",
    "            if urlparse(full_url).netloc != DOMAIN:\n",
    "                continue\n",
    "                \n",
    "            # 2. Check if we've already visited it or queued it\n",
    "            if full_url in visited_urls:\n",
    "                continue\n",
    "                \n",
    "            # 3. Ignore \"fragment\" links (like #section1)\n",
    "            if '#' in full_url:\n",
    "                continue\n",
    "                \n",
    "            # If the link is good, add it to the queue!\n",
    "            urls_to_crawl.append(full_url)\n",
    "            # Also add to visited_urls here to avoid queuing duplicates\n",
    "            visited_urls.add(full_url) \n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {current_url}: {e}\")\n",
    "        \n",
    "# --- End of while loop ---\n",
    "\n",
    "# 8. Print the final result\n",
    "print(\"--- CRAWLING COMPLETE ---\")\n",
    "print(\"Found pages:\")\n",
    "print(sitemap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
